{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import agent as ag\n",
    "import df_helpers as dfh\n",
    "import model as ml\n",
    "\n",
    "from matplotlib import style\n",
    "\n",
    "from google_window import WindowGenerator\n",
    "from model import get_lstm_model\n",
    "# style.use('dark_background')\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ag)\n",
    "importlib.reload(ml)\n",
    "importlib.reload(dfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open no1_train.csv and no1_validation.csv\n",
    "original_df_train = pd.read_csv('no1_train.csv')\n",
    "original_df_test = pd.read_csv('no1_validation.csv')\n",
    "\n",
    "# Make copies\n",
    "df_train = original_df_train.copy()\n",
    "df_test = original_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps in \"start_time\" to seconds\n",
    "df_train['start_time_seconds'] = pd.to_datetime(df_train['start_time'])\n",
    "df_train['start_time_seconds'] = df_train['start_time_seconds'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Do the same to df_test\n",
    "df_test['start_time_seconds'] = pd.to_datetime(df_test['start_time'])\n",
    "df_test['start_time_seconds'] = df_test['start_time_seconds'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First let's look at the data and see if we notice any outliers that might not correlate well with the overall trend of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as river has absolutely all values equal to 0, this is not a good feature to use. We therefore drop it because\n",
    "it gives no additional information. We will also drop sysreg since it seems this feature too is very monotonous, and does not yield much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['river'], axis=1)\n",
    "df_test = df_test.drop(['river'], axis=1)\n",
    "\n",
    "df_train = df_train.drop(['sys_reg'], axis=1)\n",
    "df_test = df_test.drop(['sys_reg'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data in several plots to see if we instinctively can see anything that doesn't add up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names from df_train except start_time\n",
    "cols = df_train.columns.drop('start_time')\n",
    "plot_features = df_train[cols]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_test\n",
    "plot_features = df_test.drop(\"start_time\", axis=1)[cols]\n",
    "plot_features.index = df_test['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two different plots; one for df_train, one for df_test with feature \"y\"\n",
    "plot_features = df_train[['y']]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_test, but in a new plot \n",
    "plot_features = df_test[['y']]\n",
    "plot_features.index = df_test['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some spikes in both datasets, especially the validation data, that seem rather inconsistent. Let's replace them with values based on the mean of the \"y\" value for other datapoints that have similar \"total\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfh.get_rows_between(df_train, 'total', 1500, 1550)\n",
    "df2 = dfh.get_rows_between(df1, 'y', -2900, 2900)\n",
    "mean_val = df2[\"y\"].mean()\n",
    "q = (df_train[\"y\"] > 1500) | (df_train[\"y\"] < -1500)\n",
    "n_clamps = df_train.loc[q].shape[0]\n",
    "print(f\"Percentage of rows clamped in training: {round(n_clamps / df_train.shape[0], 2) * 100}%\")\n",
    "df_train.loc[q, \"y\"] = mean_val\n",
    "\n",
    "# Do the same for df_test\n",
    "df1 = dfh.get_rows_between(df_test, 'total', 1500, 1550)\n",
    "df2 = dfh.get_rows_between(df1, 'y', -2900, 2900)\n",
    "mean_val = df2[\"y\"].mean()\n",
    "q = (df_test[\"y\"] > 1500) | (df_test[\"y\"] < -1500)\n",
    "n_clamps = df_test.loc[q].shape[0]\n",
    "print(f\"Percentage of rows clamped in testing: {round(n_clamps / df_test.shape[0], 2) * 100}%\")\n",
    "df_test.loc[q, \"y\"] = mean_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any NaN-values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "In this section, we will look at how we can manipulate the data in the dataset in order to better suit it for model prediction. This will include modifying existing features, and introducing new ones.\n",
    "\n",
    "First we'll implement the required feature: previous_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we'll look at frequencies.\n",
    "The nature of demand on the power grid is highly dependent on two key factors:\n",
    "* The time of day\n",
    "* The time of year\n",
    "\n",
    "Seeing as the price of electricity has been a heated debate for the past half-year due to environmental and seasonal changes, this might be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df_train['Day sin'] = np.sin(df_train['start_time_seconds'] * (2 * np.pi / day))\n",
    "df_train['Day cos'] = np.cos(df_train['start_time_seconds'] * (2 * np.pi / day))\n",
    "df_train['Year sin'] = np.sin(df_train['start_time_seconds'] * (2 * np.pi / year))\n",
    "df_train['Year cos'] = np.cos(df_train['start_time_seconds'] * (2 * np.pi / year))\n",
    "\n",
    "# Do the same, but for df_test\n",
    "df_test['Day sin'] = np.sin(df_test['start_time_seconds'] * (2 * np.pi / day))\n",
    "df_test['Day cos'] = np.cos(df_test['start_time_seconds'] * (2 * np.pi / day))\n",
    "df_test['Year sin'] = np.sin(df_test['start_time_seconds'] * (2 * np.pi / year))\n",
    "df_test['Year cos'] = np.cos(df_test['start_time_seconds'] * (2 * np.pi / year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add simple bucket-indicators for the time of day since this massively impacts the use of the power grid.\n",
    "We will use the following buckets for the feature 'time_of_day':\n",
    "\n",
    "* 0 = early morning. Times between 06:00 and 09:00\n",
    "* 1 = late morning. Times between 09:00 and 12:00\n",
    "* 2 = early day. Times between 12:00 and 15:00\n",
    "* 3 = evening. Times between 15:00 and 18:00\n",
    "* 4 = late evening. Times between 18:00 and 21:00\n",
    "* 5 = early night. Times between 21:00 and 00:00\n",
    "* 6 = night time. Times between 00:00 and 06:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hours'] = pd.to_datetime(df_train['start_time']).apply(lambda x: x.hour)\n",
    "df_train['time_of_day'] = 0\n",
    "# If time_of_day is between 6 and 8, set it to 0\n",
    "df_train.loc[(df_train['hours'] >= 6) & (df_train['hours'] <= 8), 'time_of_day'] = 0\n",
    "# If between 9 and 11, set it to 1\n",
    "df_train.loc[(df_train['hours'] >= 9) & (df_train['hours'] <= 11), 'time_of_day'] = 1\n",
    "# If between 12 and 14, set it to 2\n",
    "df_train.loc[(df_train['hours'] >= 12) & (df_train['hours'] <= 14), 'time_of_day'] = 2\n",
    "# If between 15 and 17, set it to 3\n",
    "df_train.loc[(df_train['hours'] >= 15) & (df_train['hours'] <= 17), 'time_of_day'] = 3\n",
    "# If between 18 and 20, set it to 4\n",
    "df_train.loc[(df_train['hours'] >= 18) & (df_train['hours'] <= 20), 'time_of_day'] = 4\n",
    "# If between 21 and 23, set it to 5\n",
    "df_train.loc[(df_train['hours'] >= 21) & (df_train['hours'] <= 23), 'time_of_day'] = 5\n",
    "# If between 0 and 5, set it to 6\n",
    "df_train.loc[(df_train['hours'] >= 0) & (df_train['hours'] <= 5), 'time_of_day'] = 6\n",
    "df_train.drop(columns='hours', inplace=True)\n",
    "\n",
    "# Do the same for df_test\n",
    "df_test['hours'] = pd.to_datetime(df_test['start_time']).apply(lambda x: x.hour)\n",
    "df_test['time_of_day'] = 0\n",
    "df_test.loc[(df_test['hours'] >= 6) & (df_test['hours'] <= 8), 'time_of_day'] = 0\n",
    "df_test.loc[(df_test['hours'] >= 9) & (df_test['hours'] <= 11), 'time_of_day'] = 1\n",
    "df_test.loc[(df_test['hours'] >= 12) & (df_test['hours'] <= 14), 'time_of_day'] = 2\n",
    "df_test.loc[(df_test['hours'] >= 15) & (df_test['hours'] <= 17), 'time_of_day'] = 3\n",
    "df_test.loc[(df_test['hours'] >= 18) & (df_test['hours'] <= 20), 'time_of_day'] = 4\n",
    "df_test.loc[(df_test['hours'] >= 21) & (df_test['hours'] <= 23), 'time_of_day'] = 5\n",
    "df_test.loc[(df_test['hours'] >= 0) & (df_test['hours'] <= 5), 'time_of_day'] = 6\n",
    "df_test.drop(columns='hours', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the same logic for time_of_week, however we will have to use different buckets. We'll simply add one bucket for each day of the week, i.e. 0 for monday, 1 for tuesday, etc..\n",
    "We will also add an arbitrary feature called 'weekend', which will apply to saturday and sunday (1 for 'weekend' = True, else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['time_of_week'] = pd.to_datetime(df_train['start_time']).apply(lambda x: x.weekday())\n",
    "# If time_of_week = 5 or 6, set feature 'weekend' to 1. Else set to 0\n",
    "df_train['weekend'] = 0\n",
    "df_train.loc[(df_train['time_of_week'] == 5) | (df_train['time_of_week'] == 6), 'weekend'] = 1\n",
    "\n",
    "# Do the same for df_test\n",
    "df_test['time_of_week'] = pd.to_datetime(df_test['start_time']).apply(lambda x: x.weekday())\n",
    "df_test['weekend'] = 0\n",
    "df_test.loc[(df_test['time_of_week'] == 5) | (df_test['time_of_week'] == 6), 'weekend'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to time_of_year. Here we will simply use the different months that the dates correspond to. Using this, we will also add a feature called 'season' to explicitly state whether we are in the summer, winter, autumn, or spring.\n",
    "We will bucket the season as follows using [this](https://snl.no/%C3%A5rstider) definition:\n",
    "\n",
    "* Spring will go from March through May\n",
    "* Summer will start in June, and end with (including) August\n",
    "* Autumn then starts from September, and runs until November\n",
    "* Finally, winter runs from December through February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['time_of_year'] = pd.to_datetime(df_train['start_time']).apply(lambda x: x.month)\n",
    "df_train['season'] = 0\n",
    "# If time_of_year is between 3 and 5, set it to 0\n",
    "df_train.loc[(df_train['time_of_year'] >= 3) & (df_train['time_of_year'] <= 5), 'season'] = 0\n",
    "# If between 6 and 8, set it to 1\n",
    "df_train.loc[(df_train['time_of_year'] >= 6) & (df_train['time_of_year'] <= 8), 'season'] = 1\n",
    "# If between 9 and 11, set it to 2\n",
    "df_train.loc[(df_train['time_of_year'] >= 9) & (df_train['time_of_year'] <= 11), 'season'] = 2\n",
    "# If between 12 and 2, set it to 3\n",
    "df_train.loc[df_train['time_of_year'] == 12, 'season'] = 3\n",
    "df_train.loc[df_train['time_of_year'] <= 2, 'season'] = 3\n",
    "\n",
    "# Do the same for df_test\n",
    "df_test['time_of_year'] = pd.to_datetime(df_test['start_time']).apply(lambda x: x.month)\n",
    "df_test['season'] = 0\n",
    "df_test.loc[(df_test['time_of_year'] >= 3) & (df_test['time_of_year'] <= 5), 'season'] = 0\n",
    "df_test.loc[(df_test['time_of_year'] >= 6) & (df_test['time_of_year'] <= 8), 'season'] = 1\n",
    "df_test.loc[(df_test['time_of_year'] >= 9) & (df_test['time_of_year'] <= 11), 'season'] = 2\n",
    "df_test.loc[df_test['time_of_year'] == 12, 'season'] = 3\n",
    "df_test.loc[df_test['time_of_year'] <= 2, 'season'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the features that give us the time in raw values anymore, therefore we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['start_time', 'start_time_seconds'], inplace=True)\n",
    "df_test.drop(columns=['start_time', 'start_time_seconds'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add one-hot encoding of these variables to make the dataframe more sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the columns with time_of_day, time_of_week, time_of_year, and season\n",
    "one_hot_feats = ['time_of_day', 'time_of_week', 'time_of_year', 'season']\n",
    "df_train = pd.get_dummies(df_train, columns=one_hot_feats)\n",
    "df_test = pd.get_dummies(df_test, columns=one_hot_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features did not get added to df_test since they are not observed, therefore they must be added manually\n",
    "diff_feats = [a for a in df_train.columns if a not in df_test.columns]\n",
    "df_test[diff_feats] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now just add a bunch of lagged variables. This is mainly because the model seems to heavily weight the data from the very near past, so let's just try to reinforce this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column which shifts 'total' by 24 hours\n",
    "df_train = dfh.add_shift(df_train, 'total', 24)\n",
    "df_train = dfh.add_shift(df_train, 'total', 12)\n",
    "df_train = dfh.add_shift(df_train, 'total', 6)\n",
    "# Do the same for feature 'flow'\n",
    "df_train = dfh.add_shift(df_train, 'flow', 24)\n",
    "df_train = dfh.add_shift(df_train, 'flow', 12)\n",
    "df_train = dfh.add_shift(df_train, 'flow', 6)\n",
    "\n",
    "# Do the same for df_test\n",
    "df_test = dfh.add_shift(df_test, 'total', 24)\n",
    "df_test = dfh.add_shift(df_test, 'total', 12)\n",
    "df_test = dfh.add_shift(df_test, 'total', 6)\n",
    "# Do the same for feature 'flow'\n",
    "df_test = dfh.add_shift(df_test, 'flow', 24)\n",
    "df_test = dfh.add_shift(df_test, 'flow', 12)\n",
    "df_test = dfh.add_shift(df_test, 'flow', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the feature 'previous_y', which is the imbalance from the previous timestep. We will then use this value to add a rolling average for the last 24, 12, 6, and 3 hours, meaning we get 4 additional features that measure the average 'previous_y' value based on those previous hour segments.\n",
    "\n",
    "Note that unlike ```add_shift```, ```add_shifted_target``` shifts by 1 hard-coded index, rather than a variable number of hours (this is why they are 2 different functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfh.add_shifted_target(df_train, 'y')\n",
    "df_test = dfh.add_shifted_target(df_test, 'y')\n",
    "\n",
    "df_train = dfh.add_rolling_avg(df_train, 'previous_y', hours=24)\n",
    "df_train = dfh.add_rolling_avg(df_train, 'previous_y', hours=12)\n",
    "df_train = dfh.add_rolling_avg(df_train, 'previous_y', hours=6)\n",
    "df_train = dfh.add_rolling_avg(df_train, 'previous_y', hours=3)\n",
    "\n",
    "#dftest\n",
    "df_test = dfh.add_rolling_avg(df_test, 'previous_y', hours=24)\n",
    "df_test = dfh.add_rolling_avg(df_test, 'previous_y', hours=12)\n",
    "df_test = dfh.add_rolling_avg(df_test, 'previous_y', hours=6)\n",
    "df_test = dfh.add_rolling_avg(df_test, 'previous_y', hours=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 5\n",
    "N_PREV = 24\n",
    "START_INDEX = 300\n",
    "BATCH_SIZE = 64\n",
    "TARGET = 'y'\n",
    "EPOCHS = 20\n",
    "agent = ag.Agent(\n",
    "    min_scale=-1,\n",
    "    max_scale=1,\n",
    "    resolution=RESOLUTION,\n",
    "    n_prev=N_PREV,\n",
    "    start_index=START_INDEX,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target=TARGET,\n",
    "    verbose=True,\n",
    "    model=get_lstm_model(),\n",
    "    filepath='./models/LSTM_model_1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.fit_scalers(df_train)\n",
    "df_train = agent.transform(df_train)\n",
    "df_test = agent.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all([a in df_train.columns for a in df_test.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here one can drop features to see if the model improves with less features\n",
    "# df_train.drop(columns=['Day sin', 'Day cos', 'Year sin', 'Year cos'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.train(df_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = df_test.drop(\"y\", axis=1)\n",
    "x_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = agent.predict_n_timesteps(df=x_valid, n_timesteps=1000, replace=True)\n",
    "y_true = agent.scalers['y'].inverse_transform(df_test['y'].to_numpy().reshape(-1,1))\n",
    "agent.visualize_results(y_true, y_pred, n_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "458c2d498cfecaf5a6e3710ff4ce4d06da3b56d4a5a1056796b3a3c457c6adc2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
