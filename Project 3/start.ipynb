{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import agent as ag\n",
    "import df_helpers as dfh\n",
    "import model as ml\n",
    "\n",
    "from matplotlib import style\n",
    "# style.use('dark_background')\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ag)\n",
    "importlib.reload(ml)\n",
    "importlib.reload(dfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open no1_train.csv and no1_validation.csv\n",
    "original_df_train = pd.read_csv('no1_train.csv')\n",
    "original_df_val = pd.read_csv('no1_validation.csv')\n",
    "testing = False\n",
    "try:\n",
    "    original_df_test = pd.read_csv('no1_test.csv')\n",
    "    testing = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Make copies\n",
    "df_train = original_df_train.copy()\n",
    "df_val = original_df_val.copy()\n",
    "if testing:\n",
    "    df_test = original_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps in \"start_time\" to seconds\n",
    "df_train['start_time_seconds'] = pd.to_datetime(df_train['start_time'])\n",
    "df_train['start_time_seconds'] = df_train['start_time_seconds'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Do the same to df_val\n",
    "df_val['start_time_seconds'] = pd.to_datetime(df_val['start_time'])\n",
    "df_val['start_time_seconds'] = df_val['start_time_seconds'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Do the same to df_test\n",
    "if testing:\n",
    "    df_test['start_time_seconds'] = pd.to_datetime(df_test['start_time'])\n",
    "    df_test['start_time_seconds'] = df_test['start_time_seconds'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First let's look at the data and see if we notice any outliers that might not correlate well with the overall trend of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as river has absolutely all values equal to 0, this is not a good feature to use. We therefore drop it because\n",
    "it gives no additional information. We will also drop sysreg since it seems this feature too is very monotonous, and does not yield much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['river'], axis=1)\n",
    "df_val = df_val.drop(['river'], axis=1)\n",
    "\n",
    "df_train = df_train.drop(['sys_reg'], axis=1)\n",
    "df_val = df_val.drop(['sys_reg'], axis=1)\n",
    "\n",
    "if testing:\n",
    "    df_test = df_test.drop(['river'], axis=1)\n",
    "    df_test = df_test.drop(['sys_reg'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data in several plots to see if we instinctively can see anything that doesn't add up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names from df_train except start_time\n",
    "cols = df_train.columns.drop('start_time')\n",
    "plot_features = df_train[cols]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_val\n",
    "plot_features = df_val.drop(\"start_time\", axis=1)[cols]\n",
    "plot_features.index = df_val['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two different plots; one for df_train, one for df_val with feature \"y\"\n",
    "plot_features = df_train[['y']]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_val, but in a new plot \n",
    "plot_features = df_val[['y']]\n",
    "plot_features.index = df_val['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some spikes in both datasets, especially the validation data, that seem rather inconsistent. Let's replace them with values based on the mean of the \"y\" value for other datapoints that have similar \"total\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_y(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df1 = dfh.get_rows_between(df, 'total', 1500, 1550)\n",
    "    df2 = dfh.get_rows_between(df1, 'y', -2900, 2900)\n",
    "    mean_val = df2[\"y\"].mean()\n",
    "    q = (df[\"y\"] > 1500) | (df[\"y\"] < -1500)\n",
    "    n_clamps = df.loc[q].shape[0]\n",
    "    print(f\"Percentage of rows clamped in training: {round(n_clamps / df.shape[0], 2) * 100}%\")\n",
    "    df.loc[q, \"y\"] = mean_val\n",
    "    return df\n",
    "\n",
    "df_train = clamp_y(df_train)\n",
    "df_val = clamp_y(df_val)\n",
    "if testing:\n",
    "    df_test = clamp_y(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any NaN-values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "In this section, we will look at how we can manipulate the data in the dataset in order to better suit it for model prediction. This will include modifying existing features, and introducing new ones.\n",
    "\n",
    "First we'll implement the required feature: previous_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we'll look at frequencies.\n",
    "The nature of demand on the power grid is highly dependent on two key factors:\n",
    "* The time of day\n",
    "* The time of year\n",
    "\n",
    "Seeing as the price of electricity has been a heated debate for the past half-year due to environmental and seasonal changes, this might be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_periods(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    df['Day sin'] = np.sin(df['start_time_seconds'] * (2 * np.pi / day))\n",
    "    df['Day cos'] = np.cos(df['start_time_seconds'] * (2 * np.pi / day))\n",
    "    df['Year sin'] = np.sin(df['start_time_seconds'] * (2 * np.pi / year))\n",
    "    df['Year cos'] = np.cos(df['start_time_seconds'] * (2 * np.pi / year))\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = add_day_periods(df_train)\n",
    "df_val = add_day_periods(df_val)\n",
    "if testing:\n",
    "    df_test = add_day_periods(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add simple bucket-indicators for the time of day since this massively impacts the use of the power grid.\n",
    "We will use the following buckets for the feature 'time_of_day':\n",
    "\n",
    "* 0 = early morning. Times between 06:00 and 09:00\n",
    "* 1 = late morning. Times between 09:00 and 12:00\n",
    "* 2 = early day. Times between 12:00 and 15:00\n",
    "* 3 = evening. Times between 15:00 and 18:00\n",
    "* 4 = late evening. Times between 18:00 and 21:00\n",
    "* 5 = early night. Times between 21:00 and 00:00\n",
    "* 6 = night time. Times between 00:00 and 06:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_of_day(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['hours'] = pd.to_datetime(df['start_time']).apply(lambda x: x.hour)\n",
    "    df['time_of_day'] = 0\n",
    "    # If time_of_day is between 6 and 8, set it to 0\n",
    "    df.loc[(df['hours'] >= 6) & (df['hours'] <= 8), 'time_of_day'] = 0\n",
    "    # If between 9 and 11, set it to 1\n",
    "    df.loc[(df['hours'] >= 9) & (df['hours'] <= 11), 'time_of_day'] = 1\n",
    "    # If between 12 and 14, set it to 2\n",
    "    df.loc[(df['hours'] >= 12) & (df['hours'] <= 14), 'time_of_day'] = 2\n",
    "    # If between 15 and 17, set it to 3\n",
    "    df.loc[(df['hours'] >= 15) & (df['hours'] <= 17), 'time_of_day'] = 3\n",
    "    # If between 18 and 20, set it to 4\n",
    "    df.loc[(df['hours'] >= 18) & (df['hours'] <= 20), 'time_of_day'] = 4\n",
    "    # If between 21 and 23, set it to 5\n",
    "    df.loc[(df['hours'] >= 21) & (df['hours'] <= 23), 'time_of_day'] = 5\n",
    "    # If between 0 and 5, set it to 6\n",
    "    df.loc[(df['hours'] >= 0) & (df['hours'] <= 5), 'time_of_day'] = 6\n",
    "    df.drop(columns='hours', inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = add_time_of_day(df_train)\n",
    "df_val = add_time_of_day(df_val)\n",
    "if testing:\n",
    "    df_test = add_time_of_day(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the same logic for time_of_week, however we will have to use different buckets. We'll simply add one bucket for each day of the week, i.e. 0 for monday, 1 for tuesday, etc..\n",
    "We will also add an arbitrary feature called 'weekend', which will apply to saturday and sunday (1 for 'weekend' = True, else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_of_week(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['time_of_week'] = pd.to_datetime(df['start_time']).apply(lambda x: x.weekday())\n",
    "    # If time_of_week = 5 or 6, set feature 'weekend' to 1. Else set to 0\n",
    "    df['weekend'] = 0\n",
    "    df.loc[(df['time_of_week'] == 5) | (df['time_of_week'] == 6), 'weekend'] = 1\n",
    "    return df\n",
    "\n",
    "df_train = add_time_of_week(df_train)\n",
    "df_val = add_time_of_week(df_val)\n",
    "if testing:\n",
    "    df_test = add_time_of_week(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to time_of_year. Here we will simply use the different months that the dates correspond to. Using this, we will also add a feature called 'season' to explicitly state whether we are in the summer, winter, autumn, or spring.\n",
    "We will bucket the season as follows using [this](https://snl.no/%C3%A5rstider) definition:\n",
    "\n",
    "* Spring will go from March through May\n",
    "* Summer will start in June, and end with (including) August\n",
    "* Autumn then starts from September, and runs until November\n",
    "* Finally, winter runs from December through February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_of_year(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['time_of_year'] = pd.to_datetime(df['start_time']).apply(lambda x: x.month)\n",
    "    df['season'] = 0\n",
    "    # If time_of_year is between 3 and 5, set it to 0\n",
    "    df.loc[(df['time_of_year'] >= 3) & (df['time_of_year'] <= 5), 'season'] = 0\n",
    "    # If between 6 and 8, set it to 1\n",
    "    df.loc[(df['time_of_year'] >= 6) & (df['time_of_year'] <= 8), 'season'] = 1\n",
    "    # If between 9 and 11, set it to 2\n",
    "    df.loc[(df['time_of_year'] >= 9) & (df['time_of_year'] <= 11), 'season'] = 2\n",
    "    # If between 12 and 2, set it to 3\n",
    "    df.loc[df['time_of_year'] == 12, 'season'] = 3\n",
    "    df.loc[df['time_of_year'] <= 2, 'season'] = 3\n",
    "    return df\n",
    "\n",
    "df_train = add_time_of_year(df_train)\n",
    "df_val = add_time_of_year(df_val)\n",
    "if testing:\n",
    "    df_test = add_time_of_year(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the features that give us the time in raw values anymore, therefore we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['start_time', 'start_time_seconds'], inplace=True)\n",
    "df_val.drop(columns=['start_time', 'start_time_seconds'], inplace=True)\n",
    "if testing:\n",
    "    df_test.drop(columns=['start_time', 'start_time_seconds'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add one-hot encoding of these variables to make the dataframe more sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the columns with time_of_day, time_of_week, time_of_year, and season\n",
    "one_hot_feats = ['time_of_day', 'time_of_week', 'time_of_year', 'season']\n",
    "df_train = pd.get_dummies(df_train, columns=one_hot_feats)\n",
    "df_val = pd.get_dummies(df_val, columns=one_hot_feats)\n",
    "if testing:\n",
    "    df_test = pd.get_dummies(df_test, columns=one_hot_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features did not get added to df_test since they are not observed, therefore they must be added manually\n",
    "diff_feats = [a for a in df_train.columns if a not in df_val.columns]\n",
    "df_val[diff_feats] = 0\n",
    "if testing:\n",
    "    df_test[diff_feats] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now just add a bunch of lagged variables. This is mainly because the model seems to heavily weight the data from the very near past, so let's just try to reinforce this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = [24, 12, 6]\n",
    "feats = ['total', 'flow']\n",
    "\n",
    "for hour in hours:\n",
    "    for feat in feats:\n",
    "        df_train = dfh.add_shift(df_train, feat, hour)\n",
    "        df_val = dfh.add_shift(df_val, feat, hour)\n",
    "        if testing:\n",
    "            df_test = dfh.add_shift(df_test, feat, hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the feature 'previous_y', which is the imbalance from the previous timestep. We will then use this value to add a rolling average for the last 24, 12, 6, and 3 hours, meaning we get 4 additional features that measure the average 'previous_y' value based on those previous hour segments.\n",
    "\n",
    "Note that unlike ```add_shift```, ```add_shifted_target``` shifts by 1 hard-coded index, rather than a variable number of hours (this is why they are 2 different functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfh.add_shifted_target(df_train, 'y')\n",
    "df_val = dfh.add_shifted_target(df_val, 'y')\n",
    "if testing:\n",
    "    df_test = dfh.add_shifted_target(df_test, 'y')\n",
    "\n",
    "hours = [3, 6, 12, 24]\n",
    "for h in hours:\n",
    "    df_train = dfh.add_rolling_avg(df_train, 'previous_y', h)\n",
    "    df_val = dfh.add_rolling_avg(df_val, 'previous_y', h)\n",
    "    if testing:\n",
    "        df_test = dfh.add_rolling_avg(df_test, 'previous_y', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altered forecast - predicting altered imbalance\n",
    "The final part of the feature engineering, is to create a feature called 'altered_imbalance', which is a feature corresponding to the altered forecast section of the project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dfh.add_altered_forecast(df_train, ['total', 'flow'], market_time='hourly')\n",
    "df_val = dfh.add_altered_forecast(df_val, ['total', 'flow'], market_time='hourly')\n",
    "if testing:\n",
    "    df_test = dfh.add_altered_forecast(df_test, ['total', 'flow'], market_time='hourly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally get a glimpse of what the dataframe looks like now that feature engineering is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and predictions\n",
    "In this section, we will create an agent containing a RNN model which will train on the dataset, as well as predict on validation/testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARAMETERS ARE ADJUSTED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use 5 or 15 minute resolution\n",
    "RESOLUTION = 5\n",
    "# Number of previous timesteps to train/predict on\n",
    "N_PREV = 24\n",
    "# Standard batch size\n",
    "BATCH_SIZE = 64\n",
    "# Number of epochs to train for\n",
    "EPOCHS = 100\n",
    "# Learning rate of LSTM model\n",
    "LR = 0.01\n",
    "# Number of units and layers in LSTM model\n",
    "LSTM_UNITS = [128, 64, 32]\n",
    "# Number of Dense units and layers\n",
    "DENSE_UNITS = [64, 32, 16]\n",
    "# Target column (either 'y' or 'altered_imbalance')\n",
    "TARGET = 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent uses the parameters, and then trains/predicts/visualizes/whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml.get_model(LSTM_UNITS, DENSE_UNITS, LR)\n",
    "\n",
    "agent = ag.Agent(\n",
    "    min_scale=-1,\n",
    "    max_scale=1,\n",
    "    resolution=RESOLUTION,\n",
    "    n_prev=N_PREV,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target=TARGET,\n",
    "    verbose=True,\n",
    "    model=model,\n",
    "    filepath=f'./models/{TARGET}_model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent.target == 'altered_imbalance':\n",
    "    df_train = df_train.drop(columns='y')\n",
    "    df_val = df_val.drop(columns='y')\n",
    "    if testing:\n",
    "        df_test = df_test.drop(columns='y')\n",
    "else:\n",
    "    df_train = df_train.drop(columns='altered_imbalance')\n",
    "    df_val = df_val.drop(columns='altered_imbalance')\n",
    "    if testing:\n",
    "        df_test = df_test.drop(columns='altered_imbalance')\n",
    "\n",
    "agent.fit_scalers(df_train)\n",
    "df_train = agent.transform(df_train)\n",
    "df_val = agent.transform(df_val)\n",
    "if testing:\n",
    "    df_test = agent.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.train(train=df_train, valid=df_val, epochs=EPOCHS, force_relearn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can use it to predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMESTEPS = 24\n",
    "x_valid = df_val.drop(agent.target, axis=1)\n",
    "y_true = agent.scalers[agent.target].inverse_transform(df_val[agent.target].to_numpy().reshape(-1,1))\n",
    "agent.visualize_multiple_predictions(x_valid, y_true, n_timesteps=N_TIMESTEPS, replace=True, n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    x_test = df_test.drop(agent.target, axis=1)\n",
    "    y_true = agent.scalers[agent.target].inverse_transform(df_test[agent.target].to_numpy().reshape(-1,1))\n",
    "    agent.visualize_multiple_predictions(x_test, y_true, n_timesteps=N_TIMESTEPS, replace=True, n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "458c2d498cfecaf5a6e3710ff4ce4d06da3b56d4a5a1056796b3a3c457c6adc2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
