{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import agent as ag\n",
    "import model as ml\n",
    "\n",
    "from matplotlib import style\n",
    "\n",
    "from df_helpers import get_rows_between, get_outliers\n",
    "from google_window import WindowGenerator\n",
    "from model import get_lstm_model\n",
    "style.use('dark_background')\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ag)\n",
    "importlib.reload(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open no1_train.csv and no1_validation.csv\n",
    "original_df_train = pd.read_csv('no1_train.csv')\n",
    "original_df_test = pd.read_csv('no1_validation.csv')\n",
    "\n",
    "# Make copies\n",
    "df_train = original_df_train.copy()\n",
    "df_test = original_df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps in \"start_time\" to seconds\n",
    "df_train['start_time'] = pd.to_datetime(df_train['start_time'])\n",
    "# Convert datetime into seconds since epoch\n",
    "df_train['start_time'] = df_train['start_time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# Do the same to df_test\n",
    "df_test['start_time'] = pd.to_datetime(df_test['start_time'])\n",
    "df_test['start_time'] = df_test['start_time'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First let's look at the data and see if we notice any outliers that might not correlate well with the overall trend of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as river has absolutely all values equal to 0, this is not a good feature to use. We therefore drop it because\n",
    "it gives no additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['river'], axis=1)\n",
    "df_test = df_test.drop(['river'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data in several plots to see if we instinctively can see anything that doesn't add up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names from df_train except start_time\n",
    "cols = df_train.columns.drop('start_time')\n",
    "plot_features = df_train[cols]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_test\n",
    "plot_features = df_test.drop(\"start_time\", axis=1)[cols]\n",
    "plot_features.index = df_test['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two different plots; one for df_train, one for df_test with feature \"y\"\n",
    "plot_features = df_train[['y']]\n",
    "plot_features.index = df_train['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n",
    "\n",
    "# Do the same for df_test, but in a new plot \n",
    "plot_features = df_test[['y']]\n",
    "plot_features.index = df_test['start_time']\n",
    "_ = plot_features.plot(subplots=True, figsize=(20, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some spikes in both datasets, especially the validation data, that seem rather inconsistent. Let's replace them with values based on the mean of the \"y\" value for other datapoints that have similar \"total\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_rows_between(df_train, 'total', 1500, 1550)\n",
    "df2 = get_rows_between(df1, 'y', -2900, 2900)\n",
    "mean_val = df2[\"y\"].mean()\n",
    "q1 = (df_test[\"y\"] > 1000) | (df_test[\"y\"] < -1000)\n",
    "q2 = (df_train[\"y\"] > 1000) | (df_train[\"y\"] < -1000)\n",
    "df_test.loc[q1, \"y\"] = mean_val\n",
    "df_train.loc[q2, \"y\"] = mean_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any NaN-values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "In this section, we will look at how we can manipulate the data in the dataset in order to better suit it for model prediction. This will include modifying existing features, and introducing new ones.\n",
    "\n",
    "First we'll implement the required feature: previous_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we'll look at frequencies.\n",
    "The nature of demand on the power grid is highly dependent on two key factors:\n",
    "* The time of day\n",
    "* The time of year\n",
    "\n",
    "Seeing as the price of electricity has been a heated debate for the past half-year due to environmental and seasonal changes, this might be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df_train['Day sin'] = np.sin(df_train['start_time'] * (2 * np.pi / day))\n",
    "df_train['Day cos'] = np.cos(df_train['start_time'] * (2 * np.pi / day))\n",
    "df_train['Year sin'] = np.sin(df_train['start_time'] * (2 * np.pi / year))\n",
    "df_train['Year cos'] = np.cos(df_train['start_time'] * (2 * np.pi / year))\n",
    "\n",
    "# Do the same, but for df_test\n",
    "df_test['Day sin'] = np.sin(df_test['start_time'] * (2 * np.pi / day))\n",
    "df_test['Day cos'] = np.cos(df_test['start_time'] * (2 * np.pi / day))\n",
    "df_test['Year sin'] = np.sin(df_test['start_time'] * (2 * np.pi / year))\n",
    "df_test['Year cos'] = np.cos(df_test['start_time'] * (2 * np.pi / year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 5\n",
    "N_PREV = 12\n",
    "START_INDEX = 12\n",
    "BATCH_SIZE = 32\n",
    "TARGET = 'y'\n",
    "EPOCHS = 1\n",
    "agent = ag.Agent(\n",
    "    min_scale=0,\n",
    "    max_scale=1,\n",
    "    resolution=RESOLUTION,\n",
    "    n_prev=N_PREV,\n",
    "    start_index=START_INDEX,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target=TARGET,\n",
    "    verbose=False,\n",
    "    filepath='./models/LSTM_model_1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_lstm_model()\n",
    "df_train = agent.add_previous_y_to_df(df_train, training=True)\n",
    "df_test = agent.add_previous_y_to_df(df_test, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns='start_time', inplace=True)\n",
    "df_test.drop(columns='start_time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.fit_scalers_to_df(df_train)\n",
    "history = agent.train(df_train, model=model, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = df_test.drop(\"y\", axis=1)\n",
    "y_pred = agent.predict_n_timesteps(df=x_valid, model=model, n_timesteps=300)\n",
    "agent.visualize_results(df_test[\"y\"], y_pred, n_timesteps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "458c2d498cfecaf5a6e3710ff4ce4d06da3b56d4a5a1056796b3a3c457c6adc2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
